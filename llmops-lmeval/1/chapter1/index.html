<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Course: Evaluating Model Accuracy with lm-eval-harness :: Evaluating Model Accuracy with LM-Eval-Harness</title>
    <link rel="prev" href="../LABENV/index.html">
    <link rel="next" href="section1.html">
    <meta name="generator" content="Antora 3.1.3">
    <link rel="stylesheet" href="../../../_/css/site.css">
    <script>var uiRootPath = '../../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://www.redhat.com" target="_blank"><img src="../../../_/img/redhat-logo.png" height="40px" alt="Red Hat"></a>
      <a class="navbar-item" style="font-size: 24px; color: white" href="../../..">Evaluating Model Accuracy with LM-Eval-Harness</a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <a class="navbar-item" href="https://github.com/RedHatQuickCourses/REPLACEREPONAME/issues" target="_blank">Report Issues</a>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="llmops-lmeval" data-version="1">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../index.html">Evaluating Model Accuracy with LM-Eval-Harness</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../index.html">Home</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../LABENV/index.html">Lab Environment</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item is-current-page" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="index.html">Course: Evaluating Model Accuracy with lm-eval-harness</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="section1.html">Module 1: Setting Up the Trusty AI Environment</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="section2.html">Module 2: Running a Standard Evaluation Job</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="section3.html">Module 3: Interpreting Accuracy Results</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="section4.html">Module 4: Running a Domain-Specific Test</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="section5.html">Course Wrap-up</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter2/index.html">Chapter 2</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter2/section1.html">Section 1</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter3/index.html">Chapter 3</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter3/section1.html">Section 1</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter3/section2.html">Section 2</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../appendix/appendix.html">Appendix</a>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">Evaluating Model Accuracy with LM-Eval-Harness</span>
    <span class="version">1</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="../index.html">Evaluating Model Accuracy with LM-Eval-Harness</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../index.html">1</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="../index.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../index.html">Evaluating Model Accuracy with LM-Eval-Harness</a></li>
    <li><a href="index.html">Course: Evaluating Model Accuracy with lm-eval-harness</a></li>
  </ul>
</nav>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">Course: Evaluating Model Accuracy with lm-eval-harness</h1>
<div class="sect1">
<h2 id="_introduction_to_model_accuracy_evaluation"><a class="anchor" href="#_introduction_to_model_accuracy_evaluation"></a>Introduction to Model Accuracy Evaluation</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Welcome to the course on Evaluating Model Accuracy with <code>lm-eval-harness</code>. While our previous course focused on system performance, this course will teach you how to measure a model&#8217;s <strong>quality</strong> and <strong>reasoning capabilities</strong>.</p>
</div>
<div class="paragraph">
<p>By the end of this course, you will be able to:
* Use the Trusty AI operator to run standardized model evaluations.
* Configure and launch an <code>LMEvalJob</code> to test a deployed model against a benchmark like MMLU-Pro.
* Retrieve and interpret the accuracy results to understand a model&#8217;s strengths and weaknesses.
* Run domain-specific tests to validate a model&#8217;s knowledge in a particular field.</p>
</div>
<div class="sect2">
<h3 id="_what_is_lm_eval_harness"><a class="anchor" href="#_what_is_lm_eval_harness"></a>What is lm-eval-harness?</h3>
<div class="paragraph">
<p><strong>lm-eval-harness</strong> is a widely adopted, community-maintained benchmarking toolkit from <strong>EleutherAI</strong>. It provides a standardized framework for evaluating Large Language Models (LLMs) across dozens of academic benchmarks, ensuring consistent and reproducible results. It can test models on tasks ranging from question answering and common sense reasoning to complex, multi-subject examinations.</p>
</div>
</div>
<div class="sect2">
<h3 id="_what_is_trusty_ai"><a class="anchor" href="#_what_is_trusty_ai"></a>What is Trusty AI?</h3>
<div class="paragraph">
<p><a href="https://trustyai.org/docs/main/main" target="_blank" rel="noopener">TrustyAI</a> is Red Hat&#8217;s open-source AI explainability and trustworthiness platform. Within OpenShift AI, Trusty AI provides an enterprise-ready, operator-based way to run evaluation frameworks like <code>lm-eval-harness</code>. It simplifies the process of running evaluation jobs in a secure and manageable way. For this lab, we will use the Trusty AI operator to schedule and manage our accuracy tests.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_module_1_setting_up_the_trusty_ai_environment"><a class="anchor" href="#_module_1_setting_up_the_trusty_ai_environment"></a>Module 1: Setting Up the Trusty AI Environment</h2>
<div class="sectionbody">
<div class="paragraph">
<p>In this module, we will enable and configure the Trusty AI operator in OpenShift AI to prepare it for running evaluation jobs.</p>
</div>
<div class="sect2">
<h3 id="_1_1_enable_the_trusty_ai_component"><a class="anchor" href="#_1_1_enable_the_trusty_ai_component"></a>1.1 Enable the Trusty AI Component</h3>
<div class="paragraph">
<p>First, we&#8217;ll patch the default <code>DataScienceCluster</code> resource to change the management state of the <code>trustyai</code> component from "Removed" to "Managed".</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-console hljs" data-lang="console">oc patch datasciencecluster default-dsc -p '{"spec":{"components":{"trustyai":{"managementState":"Managed"}}}}' --type=merge</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_1_2_configure_for_remote_dataset_access"><a class="anchor" href="#_1_2_configure_for_remote_dataset_access"></a>1.2 Configure for Remote Dataset Access</h3>
<div class="paragraph">
<p>By default, Trusty AI jobs run in a sandboxed environment without internet access. To run standard benchmarks, we need to allow the job to download the evaluation dataset and the model&#8217;s tokenizer from Hugging Face.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>We will patch the Trusty AI <code>ConfigMap</code> to allow online access and code execution from the downloaded datasets.</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-console hljs" data-lang="console">oc patch configmap trustyai-service-operator-config -n redhat-ods-applications  \
--type merge -p '{"metadata": {"annotations": {"opendatahub.io/managed": "false"}}}'

oc patch configmap trustyai-service-operator-config -n redhat-ods-applications \
--type merge -p '{"data":{"lmes-allow-online":"true","lmes-allow-code-execution":"true"}}'</code></pre>
</div>
</div>
</li>
<li>
<p>After patching the <code>ConfigMap</code>, we need to restart the operator for the changes to take effect.</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-console hljs" data-lang="console">oc rollout restart deployment trustyai-service-operator-controller-manager -n redhat-ods-applications</code></pre>
</div>
</div>
</li>
</ol>
</div>
<div class="paragraph">
<p>Wait for the <code>trustyai-service-operator-controller-manager</code> pod to restart. Once it is running, your environment is ready for evaluation.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_module_2_running_a_standard_evaluation_job"><a class="anchor" href="#_module_2_running_a_standard_evaluation_job"></a>Module 2: Running a Standard Evaluation Job</h2>
<div class="sectionbody">
<div class="paragraph">
<p>With the environment ready, we can now define and run our first evaluation. We will test our <code>granite-8b</code> model against the <strong>ARC Easy</strong> benchmark, a grade-school science reasoning test.</p>
</div>
<div class="sect2">
<h3 id="_2_1_configure_the_evaluation_job"><a class="anchor" href="#_2_1_configure_the_evaluation_job"></a>2.1 Configure the Evaluation Job</h3>
<div class="paragraph">
<p>The evaluation is defined in a YAML file that specifies the <code>LMEvalJob</code> custom resource. The most important step is to point the job to your model&#8217;s inference endpoint.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Edit the <code>workshop_code/evals/trusty/arc_easy.yaml</code> file and update the <code>base_url</code> value with your model&#8217;s external inference URL.</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">- name: base_url
      # the location of your model's /chat/completions or /completions endpoint
      value: https://&lt;YOUR_EXTERNAL_INFERENCE_ENDPOINT&gt;/v1/completions</code></pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect2">
<h3 id="_2_2_create_and_run_the_job"><a class="anchor" href="#_2_2_create_and_run_the_job"></a>2.2 Create and Run the Job</h3>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Apply the YAML file to your cluster to create the <code>LMEvalJob</code> resource. This will trigger Trusty AI to start a new evaluation pod.</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-console hljs" data-lang="console">oc apply -f workshop_code/evals/trusty/arc_easy.yaml -n vllm</code></pre>
</div>
</div>
</li>
<li>
<p>You can watch the progress of the evaluation by tailing the logs of the job pod. You will see progress reported in percentage points. This run will take approximately 10 minutes.</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-console hljs" data-lang="console">watch oc logs -f arc-easy-eval-job -n vllm</code></pre>
</div>
</div>
</li>
<li>
<p>Alternatively, you can watch the logs of your model&#8217;s predictor pod to see the actual questions being sent from the evaluation harness.</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-console hljs" data-lang="console"># Find your exact pod name first
oc logs -f granite-8b-predictor-&lt;exact-pod-name&gt; -n vllm</code></pre>
</div>
</div>
</li>
</ol>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_module_3_interpreting_accuracy_results"><a class="anchor" href="#_module_3_interpreting_accuracy_results"></a>Module 3: Interpreting Accuracy Results</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Once the evaluation job is complete, the results are stored directly in the <code>LMEvalJob</code> custom resource. This module explains how to retrieve and understand these results.</p>
</div>
<div class="sect2">
<h3 id="_3_1_retrieve_the_results"><a class="anchor" href="#_3_1_retrieve_the_results"></a>3.1 Retrieve the Results</h3>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Use <code>oc get</code> with a <code>jsonpath</code> template to extract the results from the job&#8217;s status field. We pipe it to <code>jq</code> for clean formatting.</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-console hljs" data-lang="console">oc get LMEvalJob arc-easy-eval-job -n vllm -o jsonpath='{.status.results}' | jq '.results'</code></pre>
</div>
</div>
</li>
<li>
<p>The command will return a JSON object similar to this:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-json hljs" data-lang="json">{
  "arc_easy": {
    "alias": "arc_easy",
    "acc,none": 0.8186026936026936,
    "acc_stderr,none": 0.007907153952801706,
    "acc_norm,none": 0.7836700336700336,
    "acc_norm_stderr,none": 0.00844876352205705
  }
}</code></pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect2">
<h3 id="_3_2_explanation_of_metrics"><a class="anchor" href="#_3_2_explanation_of_metrics"></a>3.2 Explanation of Metrics</h3>
<div class="ulist">
<ul>
<li>
<p><code>acc,none</code>: This is the primary <strong>accuracy</strong> score. A value of <code>0.8186</code> means the model answered approximately 81.86% of questions correctly based on its raw, unmodified output.</p>
</li>
<li>
<p><code>acc_norm,none</code>: This is the <strong>normalized accuracy</strong>. It represents the accuracy after the model&#8217;s answers have been cleaned up (e.g., removing extra spaces or standardizing capitalization). This is often a more realistic measure of performance. Here, it is 78.37%.</p>
</li>
<li>
<p><code>acc_stderr,none</code>: This is the <strong>standard error</strong> for the accuracy score. It represents the margin of error, indicating how much the result might vary on a different sample of questions. A smaller number means the result is more statistically reliable.</p>
</li>
<li>
<p><code>acc_norm_stderr,none</code>: This is the standard error for the normalized accuracy.</p>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_module_4_running_a_domain_specific_test"><a class="anchor" href="#_module_4_running_a_domain_specific_test"></a>Module 4: Running a Domain-Specific Test</h2>
<div class="sectionbody">
<div class="paragraph">
<p>General benchmarks are useful, but for many enterprise use cases, you need to validate a model&#8217;s knowledge in a specific domain. In this module, we will run a test focused on jurisprudence (the study of law) from the MMLU benchmark suite.</p>
</div>
<div class="sect2">
<h3 id="_4_1_run_the_jurisprudence_evaluation"><a class="anchor" href="#_4_1_run_the_jurisprudence_evaluation"></a>4.1 Run the Jurisprudence Evaluation</h3>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>First, ensure the <code>base_url</code> in the <code>workshop_code/evals/trusty/mmlu_jurisprudence.yaml</code> file is updated with your inference endpoint.</p>
</li>
<li>
<p>Apply the new <code>LMEvalJob</code> file. This test is much shorter and should only take a minute or two.</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-console hljs" data-lang="console">oc apply -f workshop_code/evals/trusty/mmlu_jurisprudence.yaml -n vllm</code></pre>
</div>
</div>
</li>
<li>
<p>Once the job is complete, retrieve the results using the same method as before, but targeting the new job name.</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-console hljs" data-lang="console">oc get LMEvalJob mmlu-jurisprudence-eval-job -n vllm -o template --template '{{.status.results}}' | jq  .results</code></pre>
</div>
</div>
</li>
</ol>
</div>
<div class="paragraph">
<p>This process demonstrates a powerful pattern: you can quickly validate a model&#8217;s capabilities against specific subject areas to ensure it is suitable for a specialized task.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_course_wrap_up"><a class="anchor" href="#_course_wrap_up"></a>Course Wrap-up</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Congratulations! You have successfully evaluated a deployed AI model&#8217;s accuracy using an industry-standard framework on OpenShift AI.</p>
</div>
<div class="sect2">
<h3 id="_summary_of_learnings"><a class="anchor" href="#_summary_of_learnings"></a>Summary of Learnings</h3>
<div class="paragraph">
<p>In this course, we demonstrated a complete workflow for model accuracy validation.</p>
</div>
<div class="paragraph">
<p>What We Did:
* <strong>Set up the TrustyAI operator</strong> to enable the model evaluation framework in OpenShift AI.
* <strong>Configured internet access</strong> to allow the downloading of evaluation datasets from Hugging Face.
* <strong>Ran the ARC Easy benchmark</strong> by creating an <code>LMEvalJob</code> to test the model&#8217;s reasoning capabilities.
* <strong>Analyzed the accuracy results</strong>, learning the difference between raw and normalized accuracy.
* <strong>Ran a domain-specific test</strong> to validate the model&#8217;s knowledge on a specific subject.</p>
</div>
<div class="paragraph">
<p>The key outcome is that you can now measure and validate AI model accuracy in a production-like environment using automated, repeatable evaluation pipelines. This is a critical skill for selecting the right model and ensuring its quality over time.</p>
</div>
</div>
</div>
</div>
<nav class="pagination">
  <span class="prev"><a href="../LABENV/index.html">Lab Environment</a></span>
  <span class="next"><a href="section1.html">Module 1: Setting Up the Trusty AI Environment</a></span>
</nav>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <img src="../../../_/img/rhl-logo-red.png" height="40px" alt="Red Hat"  href="https://redhat.com" >
</footer><script id="site-script" src="../../../_/js/site.js" data-ui-root-path="../../../_"></script>
<script async src="../../../_/js/vendor/highlight.js"></script>
  </body>
</html>
