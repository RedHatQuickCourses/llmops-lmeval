<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Course: Evaluating Model Accuracy with lm-eval-harness :: Evaluating Model Accuracy with LM-Eval-Harness</title>
    <link rel="prev" href="../LABENV/index.html">
    <link rel="next" href="section1.html">
    <meta name="generator" content="Antora 3.1.3">
    <link rel="stylesheet" href="../../../_/css/site.css">
    <script>var uiRootPath = '../../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://www.redhat.com" target="_blank"><img src="../../../_/img/redhat-logo.png" height="40px" alt="Red Hat"></a>
      <a class="navbar-item" style="font-size: 24px; color: white" href="../../..">Evaluating Model Accuracy with LM-Eval-Harness</a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <a class="navbar-item" href="https://github.com/RedHatQuickCourses/REPLACEREPONAME/issues" target="_blank">Report Issues</a>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="llmops-lmeval" data-version="1">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../index.html">Evaluating Model Accuracy with LM-Eval-Harness</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../index.html">Home</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../LABENV/index.html">Lab Environment</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item is-current-page" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="index.html">Course: Evaluating Model Accuracy with lm-eval-harness</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="section1.html">Module 1: Setting Up the Trusty AI Environment</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="section2.html">Module 2: Running a Standard Evaluation Job</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="section3.html">Module 3: Interpreting Accuracy Results</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="section4.html">Module 4: Running a Domain-Specific Test</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="section5.html">Course Wrap-up</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">Evaluating Model Accuracy with LM-Eval-Harness</span>
    <span class="version">1</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="../index.html">Evaluating Model Accuracy with LM-Eval-Harness</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../index.html">1</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="../index.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../index.html">Evaluating Model Accuracy with LM-Eval-Harness</a></li>
    <li><a href="index.html">Course: Evaluating Model Accuracy with lm-eval-harness</a></li>
  </ul>
</nav>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">Course: Evaluating Model Accuracy with lm-eval-harness</h1>
<div class="sect1">
<h2 id="_introduction_to_model_accuracy_evaluation"><a class="anchor" href="#_introduction_to_model_accuracy_evaluation"></a>Introduction to Model Accuracy Evaluation</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Welcome to the course on Evaluating Model Accuracy with <code>lm-eval-harness</code>. While our previous course focused on system performance, this course will teach you how to measure a model&#8217;s <strong>quality</strong> and <strong>reasoning capabilities</strong>.</p>
</div>
<div class="paragraph">
<p>By the end of this course, you will be able to:
* Use the Trusty AI operator to run standardized model evaluations.
* Configure and launch an <code>LMEvalJob</code> to test a deployed model against a benchmark like MMLU-Pro.
* Retrieve and interpret the accuracy results to understand a model&#8217;s strengths and weaknesses.
* Run domain-specific tests to validate a model&#8217;s knowledge in a particular field.</p>
</div>
<div class="sect2">
<h3 id="_what_is_lm_eval_harness"><a class="anchor" href="#_what_is_lm_eval_harness"></a>What is lm-eval-harness?</h3>
<div class="paragraph">
<p><strong>lm-eval-harness</strong> is a widely adopted, community-maintained benchmarking toolkit from <strong>EleutherAI</strong>. It provides a standardized framework for evaluating Large Language Models (LLMs) across dozens of academic benchmarks, ensuring consistent and reproducible results. It can test models on tasks ranging from question answering and common sense reasoning to complex, multi-subject examinations.</p>
</div>
</div>
<div class="sect2">
<h3 id="_what_is_trusty_ai"><a class="anchor" href="#_what_is_trusty_ai"></a>What is Trusty AI?</h3>
<div class="paragraph">
<p><a href="https://trustyai.org/docs/main/main" target="_blank" rel="noopener">TrustyAI</a> is Red Hat&#8217;s open-source AI explainability and trustworthiness platform. Within OpenShift AI, Trusty AI provides an enterprise-ready, operator-based way to run evaluation frameworks like <code>lm-eval-harness</code>. It simplifies the process of running evaluation jobs in a secure and manageable way. For this lab, we will use the Trusty AI operator to schedule and manage our accuracy tests.</p>
</div>
</div>
</div>
</div>
<nav class="pagination">
  <span class="prev"><a href="../LABENV/index.html">Lab Environment</a></span>
  <span class="next"><a href="section1.html">Module 1: Setting Up the Trusty AI Environment</a></span>
</nav>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <img src="../../../_/img/rhl-logo-red.png" height="40px" alt="Red Hat"  href="https://redhat.com" >
</footer><script id="site-script" src="../../../_/js/site.js" data-ui-root-path="../../../_"></script>
<script async src="../../../_/js/vendor/highlight.js"></script>
  </body>
</html>
