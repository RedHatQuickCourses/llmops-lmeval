= Course: Evaluating Model Accuracy with lm-eval-harness


// -- Page Break --

== Introduction to Model Accuracy Evaluation

Welcome to the course on Evaluating Model Accuracy with `lm-eval-harness`. While our previous course focused on system performance, this course will teach you how to measure a model's *quality* and *reasoning capabilities*.

By the end of this course, you will be able to:
* Use the Trusty AI operator to run standardized model evaluations.
* Configure and launch an `LMEvalJob` to test a deployed model against a benchmark like MMLU-Pro.
* Retrieve and interpret the accuracy results to understand a model's strengths and weaknesses.
* Run domain-specific tests to validate a model's knowledge in a particular field.

=== What is lm-eval-harness?

**lm-eval-harness** is a widely adopted, community-maintained benchmarking toolkit from **EleutherAI**. It provides a standardized framework for evaluating Large Language Models (LLMs) across dozens of academic benchmarks, ensuring consistent and reproducible results. It can test models on tasks ranging from question answering and common sense reasoning to complex, multi-subject examinations.

=== What is Trusty AI?

https://trustyai.org/docs/main/main[TrustyAI^] is Red Hat's open-source AI explainability and trustworthiness platform. Within OpenShift AI, Trusty AI provides an enterprise-ready, operator-based way to run evaluation frameworks like `lm-eval-harness`. It simplifies the process of running evaluation jobs in a secure and manageable way. For this lab, we will use the Trusty AI operator to schedule and manage our accuracy tests.

// -- Page Break --

== Module 1: Setting Up the Trusty AI Environment

In this module, we will enable and configure the Trusty AI operator in OpenShift AI to prepare it for running evaluation jobs.

=== 1.1 Enable the Trusty AI Component

First, we'll patch the default `DataScienceCluster` resource to change the management state of the `trustyai` component from "Removed" to "Managed".

[source,console,role=execute,subs=attributes+]
----
oc patch datasciencecluster default-dsc -p '{"spec":{"components":{"trustyai":{"managementState":"Managed"}}}}' --type=merge
----

=== 1.2 Configure for Remote Dataset Access

By default, Trusty AI jobs run in a sandboxed environment without internet access. To run standard benchmarks, we need to allow the job to download the evaluation dataset and the model's tokenizer from Hugging Face.

. We will patch the Trusty AI `ConfigMap` to allow online access and code execution from the downloaded datasets.
+
[source,console,role=execute,subs=attributes+]
----
oc patch configmap trustyai-service-operator-config -n redhat-ods-applications  \
--type merge -p '{"metadata": {"annotations": {"opendatahub.io/managed": "false"}}}'

oc patch configmap trustyai-service-operator-config -n redhat-ods-applications \
--type merge -p '{"data":{"lmes-allow-online":"true","lmes-allow-code-execution":"true"}}'
----

. After patching the `ConfigMap`, we need to restart the operator for the changes to take effect.
+
[source,console,role=execute,subs=attributes+]
----
oc rollout restart deployment trustyai-service-operator-controller-manager -n redhat-ods-applications
----

Wait for the `trustyai-service-operator-controller-manager` pod to restart. Once it is running, your environment is ready for evaluation.

// -- Page Break --

== Module 2: Running a Standard Evaluation Job

With the environment ready, we can now define and run our first evaluation. We will test our `granite-8b` model against the **ARC Easy** benchmark, a grade-school science reasoning test.

=== 2.1 Configure the Evaluation Job

The evaluation is defined in a YAML file that specifies the `LMEvalJob` custom resource. The most important step is to point the job to your model's inference endpoint.

. Edit the `workshop_code/evals/trusty/arc_easy.yaml` file and update the `base_url` value with your model's external inference URL.
+
[source,yaml]
----
- name: base_url
      # the location of your model's /chat/completions or /completions endpoint
      value: https://<YOUR_EXTERNAL_INFERENCE_ENDPOINT>/v1/completions
----

=== 2.2 Create and Run the Job

. Apply the YAML file to your cluster to create the `LMEvalJob` resource. This will trigger Trusty AI to start a new evaluation pod.
+
[source,console,role=execute,subs=attributes+]
----
oc apply -f workshop_code/evals/trusty/arc_easy.yaml -n vllm
----

. You can watch the progress of the evaluation by tailing the logs of the job pod. You will see progress reported in percentage points. This run will take approximately 10 minutes.
+
[source,console,role=execute,subs=attributes+]
----
watch oc logs -f arc-easy-eval-job -n vllm
----

. Alternatively, you can watch the logs of your model's predictor pod to see the actual questions being sent from the evaluation harness.
+
[source,console,role=execute,subs=attributes+]
----
# Find your exact pod name first
oc logs -f granite-8b-predictor-<exact-pod-name> -n vllm
----

// -- Page Break --

== Module 3: Interpreting Accuracy Results

Once the evaluation job is complete, the results are stored directly in the `LMEvalJob` custom resource. This module explains how to retrieve and understand these results.

=== 3.1 Retrieve the Results

. Use `oc get` with a `jsonpath` template to extract the results from the job's status field. We pipe it to `jq` for clean formatting.
+
[source,console,role=execute,subs=attributes+]
----
oc get LMEvalJob arc-easy-eval-job -n vllm -o jsonpath='{.status.results}' | jq '.results'
----

. The command will return a JSON object similar to this:
+
[source,json]
----
{
  "arc_easy": {
    "alias": "arc_easy",
    "acc,none": 0.8186026936026936,
    "acc_stderr,none": 0.007907153952801706,
    "acc_norm,none": 0.7836700336700336,
    "acc_norm_stderr,none": 0.00844876352205705
  }
}
----

=== 3.2 Explanation of Metrics

* `acc,none`: This is the primary **accuracy** score. A value of `0.8186` means the model answered approximately 81.86% of questions correctly based on its raw, unmodified output.
* `acc_norm,none`: This is the **normalized accuracy**. It represents the accuracy after the model's answers have been cleaned up (e.g., removing extra spaces or standardizing capitalization). This is often a more realistic measure of performance. Here, it is 78.37%.
* `acc_stderr,none`: This is the **standard error** for the accuracy score. It represents the margin of error, indicating how much the result might vary on a different sample of questions. A smaller number means the result is more statistically reliable.
* `acc_norm_stderr,none`: This is the standard error for the normalized accuracy.

// -- Page Break --

== Module 4: Running a Domain-Specific Test

General benchmarks are useful, but for many enterprise use cases, you need to validate a model's knowledge in a specific domain. In this module, we will run a test focused on jurisprudence (the study of law) from the MMLU benchmark suite.

=== 4.1 Run the Jurisprudence Evaluation

. First, ensure the `base_url` in the `workshop_code/evals/trusty/mmlu_jurisprudence.yaml` file is updated with your inference endpoint.

. Apply the new `LMEvalJob` file. This test is much shorter and should only take a minute or two.
+
[source,console,role=execute,subs=attributes+]
----
oc apply -f workshop_code/evals/trusty/mmlu_jurisprudence.yaml -n vllm
----

. Once the job is complete, retrieve the results using the same method as before, but targeting the new job name.
+
[source,console,role=execute,subs=attributes+]
----
oc get LMEvalJob mmlu-jurisprudence-eval-job -n vllm -o template --template '{{.status.results}}' | jq  .results
----

This process demonstrates a powerful pattern: you can quickly validate a model's capabilities against specific subject areas to ensure it is suitable for a specialized task.

// -- Page Break --

== Course Wrap-up

Congratulations! You have successfully evaluated a deployed AI model's accuracy using an industry-standard framework on OpenShift AI.

=== Summary of Learnings

In this course, we demonstrated a complete workflow for model accuracy validation.

What We Did:
* **Set up the TrustyAI operator** to enable the model evaluation framework in OpenShift AI.
* **Configured internet access** to allow the downloading of evaluation datasets from Hugging Face.
* **Ran the ARC Easy benchmark** by creating an `LMEvalJob` to test the model's reasoning capabilities.
* **Analyzed the accuracy results**, learning the difference between raw and normalized accuracy.
* **Ran a domain-specific test** to validate the model's knowledge on a specific subject.

The key outcome is that you can now measure and validate AI model accuracy in a production-like environment using automated, repeatable evaluation pipelines. This is a critical skill for selecting the right model and ensuring its quality over time.
