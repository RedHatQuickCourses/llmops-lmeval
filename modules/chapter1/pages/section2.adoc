= Module 2: Running a Standard Evaluation Job

With the environment ready, we can now define and run our first evaluation. We will test our `granite-8b` model against the **ARC Easy** benchmark, a grade-school science reasoning test.

=== 2.1 Configure the Evaluation Job

The evaluation is defined in a YAML file that specifies the `LMEvalJob` custom resource. The most important step is to point the job to your model's inference endpoint.

. Edit the `workshop_code/evals/trusty/arc_easy.yaml` file and update the `base_url` value with your model's external inference URL.
+
[source,yaml]
----
- name: base_url
      # the location of your model's /chat/completions or /completions endpoint
      value: https://<YOUR_EXTERNAL_INFERENCE_ENDPOINT>/v1/completions
----

=== 2.2 Create and Run the Job

. Apply the YAML file to your cluster to create the `LMEvalJob` resource. This will trigger Trusty AI to start a new evaluation pod.
+
[source,console,role=execute,subs=attributes+]
----
oc apply -f workshop_code/evals/trusty/arc_easy.yaml -n vllm
----

. You can watch the progress of the evaluation by tailing the logs of the job pod. You will see progress reported in percentage points. This run will take approximately 10 minutes.
+
[source,console,role=execute,subs=attributes+]
----
watch oc logs -f arc-easy-eval-job -n vllm
----

. Alternatively, you can watch the logs of your model's predictor pod to see the actual questions being sent from the evaluation harness.
+
[source,console,role=execute,subs=attributes+]
----
# Find your exact pod name first
oc logs -f granite-8b-predictor-<exact-pod-name> -n vllm
----